# Learning Objectives

By the end of this chapter, you should be able to:
- Display and append to file contents using **cat** and **echo**. 
- Edit and print file contents using **sed** and **awk**.
- Search for patterns using **grep**.
- Use multiple other utilities for file and text manipulation.
# Command Line Tools for Manipulating Text Files
- irrespective of the role you play with Linux (system administrator, developer, or user), you often need to browse through and parse text files and/or extract data from them
- thus, it is essential for Linux users functioning in any of these capacities to become adept at performing these file manipulation operations
- most of the time, such file manipulation is done at the command line, which allows users to perform tasks more efficiently than while using a GUI
- furthermore, the command line is more suitable for ==automating often-executed tasks==
- indeed, experienced system administrators write customized scripts to accomplish such repetitive tasks, standardized for each particular environment
## cat
- "cat" is short for concatenate
- one of the most frequently used Linux command line utilities
- it is often used to read and print files, as well as for simply viewing file contents
- to view a file, use the ff. command:
	- $ cat \<filename\>
- for example, "cat readme.txt" will display the contents of "readme.txt" on the terminal
- however, ==the main purpose of "cat" is often to combine (concatenate) multiple files together==
- the "tac" command prints the lines of a file in reverse order
	- each line remains the same, but the order of the lines is inverted
	- the syntax of "tac" is exactly the same as for "cat", as in:
		- $ tac file
		- $ tac file1 file2 > newfile
- common usage for "cat" (and "tac"):

| COMMAND                   | USAGE                                                                                                                                  |
| ------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |
| cat file1 file2           | concatenate multiple files and display the output<br>(ie. the entire content of the first file is followed by that of the second file) |
| cat file1 file2 > newfile | combine multiple files and save the output into a new file                                                                             |
| cat file >> existingfile  | append a file to the end of an existing file                                                                                           |
| cat > file                | any subsequent lines typed will go into the file, until CTRL-D is typed                                                                |
| cat >> file               | any subsequent lines are appended to the file, until CTRL-D is typed                                                                   |
|                           |                                                                                                                                        |
### using cat interactively
- "cat" can be used to read from standard input (such as the terminal window) if no files are specified
- you can use the '>' operator to create and add lines into a new file, and the ">>" operator to append lines (or files) to an existing file
- to create a new file, at the command prompt, type:
	- $ cat > \<filename\>
	- this command creates a new file and waits for the user to edit/enter the text
	- after you finish typing the required text, press CTRL-D at the beginning of the next line to save and exit the editing
- another way to create a file at the terminal is:
	- $ cat > \<filename\> << EOF
	- a new file is created and you can type the required input
	- to exit, enter EOF at the beginning of a line
	- note that EOF is case sensitive
	- one can also use another word, such as STOP
## echo
- "echo" simply displays (echoes) text
- it is used simply, as in:
	- $ echo string
- "echo" can be used to:
	- display a string on standard output (ie. the terminal)
	- place a string in a new file (using the '>' operator)
	- append a string to an already existing file (using the ">>" operator)
- the "-e" option, along with the ff. switches, is used to enable special character sequences, such as the newline character or horizontal tab:
	- \n represents newline
	- \t represents horizontal tab
- "echo" is particularly useful for viewing the values of environment variables (built-in shell variables) 
	- eg. "echo $USERNAME" will print the name of the user who has logged into the current terminal
- common usage for "echo":

| COMMAND                     | USAGE                                                                   |
| --------------------------- | ----------------------------------------------------------------------- |
| echo string > newfile       | the specified string is placed in a new file                            |
| echo string >> existingfile | the specified string is appended to the end of an already existing file |
| echo $variable              | the contents of the specified environment variable are displayed        |
## Working with Large and Compressed Files
- system administrators need to work with configuration files, text files, documentation files, and log files
- some of these files may be large or become quite large as they accumulate data with time
- these files will require both viewing and administrative updating
- eg. a system might maintain one simple large log file to record details of all system warnings and errors
	- modern systems tend to have more fine-grained logging facilities but still may have some large logging files
	- due to a security attack or a malfunction, the administrator might be forced to check for some data by navigating within the file
	- in such cases, directly opening the file in an editor will probably be inefficient (due to high memory utilization) because most text editors usually try to read the whole file into memory first
- instead, one can use =="less"== to view the contents of such a large file, scrolling up and down page by page, without the system having to place the entire file in memory before starting
	- this is much faster than using a text editor
- viewing "somefile" can be done by typing either of the two ff. commands:
	- $ less somefile
	- $ cat somefile | less
- by default, "man" pages are sent through the "less" command
- you may have encountered the older, "more" utility, which has the same basic function but fewer capabilities
	- ie. "less" is "more"!
### head
- "head" reads the first few lines of each named file (10 by default) and displays it on standard output
- you can give it a different number of lines in an option
- eg. if you want to print the first 5 lines from "/etc/default/grub", use the ff. command:
	- $ head -n 5 /etc/default/grub
	- or, omit the "-n" by typing:
		- $ head -5 /etc/default/grub
### tail
- "tail" prints the last few lines of each named file (10 by default) and displays it on standard output
- you can give it a different number of lines in an option
- "tail" is especially useful when you are troubleshooting any issue using log files, as you probably want to see the most recent lines of output
- eg. to display the last 15 lines of "somefile.log", use the ff. command:
	- $ tail -n 15 somefile.log
	- or, omit the "-n" by typing:
	- $ tail -15 somefile.log
- to continually monitor new output in a growing log file:
	- use the "-f" option
	- $ tail -f somefile.log
	- this command will continuously display any new lines of output in "somefile.log" as soon as they appear
	- thus, it enables you to monitor any current activity that is being reported and recorded
### Viewing Compressed Files
- when working with compressed files, many standard commands cannot be used directly
- for many commonly-used file and text manipulation programs, there is also a version especially designed to work directly with compressed files
- these associated utilities often have the letter "z" prefixed to their name
- eg. we have utility programs such as:
	- zcat
		- $ zcat compressed-file.txt.gz
			- to view a compressed file
	- zless
		- $ zless somefile.gz or $ zmore somefile.gz
			- to page through a compressed file
	- zdiff
		- zdiff file1.txt.gz file2.txt.gz
			- to compare two compressed files
	- zgrep
		- zgrep -i findthis somefile.gz
			- to search inside a compressed file
	- note that if you run "zless" on an uncompressed file, it will still work and ignore the decompression stage
	- there are also equivalent utility programs for other compression methods besides "gzip" (eg. "xz" or"bzip2") like:
		- xzcat, xzless, xzdiff
		- bzcat, bzless, bzdiff
## sed and awk
- it is very common to create and then repeatedly edit and/or extract contents from a file
- note that many Linux users and administrators will write scripts using comprehensive scripting languages such as Python and perl, rather than use "sed" and "awk"
	- using such utilities is certainly fine in most circumstances
	- one should always feel free to use the tools one is experienced with
	- however, the utilities that are described here are much lighter
		- ==ie. they use fewer system resources, and execute faster==
	- there are situations (such as during booting the system) where a lot of time would be wasted using the more complicated tools
		- and the system may not even be able to run them
			- so, the simpler tools will always be needed 
### sed
- "sed" is a powerful text processing tool and is one of the oldest, earliest and most popular UNIX utilities
- it is used to ==modify the contents of a file or input stream==, usually ==placing the contents into a new file or output stream==
- its name is an abbreviation of "stream editor"
- Input Stream -> Working Stream -> Output Stream
- "sed" can ==filter text==, as well as ==perform substitutions in data streams==
- data from an input source/file (or stream) is taken and moved to a working space
- the entire list of operations/modifications is applied over the data in the working space and the final contents are moved to the standard output space (or stream)
- ==Command Syntax==, you can invoke "sed" using commands like:
	- $ sed -e command \<filename\>
		- specify editing commands at the command line, process input from a file, and put the output on standard out (eg. the terminal)
	- $ sed -f scriptfile \<filename\>
		- specify a script file containing "sed" commands, operate on file, and put output on standard out
	- $ echo "I hate you" | sed s/hate/love/
		- use "sed" to filter standard input, putting output on standard out
	- the "-e" option allows you to specify multiple editing commands simultaneously at the command line
		- it is unnecessary if you only have one operation invoked
- ==basic operations==:
	- "pattern" is the current string, "replace_string" is the new string
	- ==NOTE==: you can use different delimiters in sed, '/' is just commonly used
	- $ sed s/pattern/replace_string/ file
		- substitute first string occurrence in every line
	- $ sed s/pattern/replace_string/g file
		- substitute all string occurrences in every line
	- $ sed 1,3s/pattern/replace_string/g file
		- substitute all string occurrences in a range of lines
	- $ sed -i s/pattern/replace_string/g file
		- save changes for string substitution in the same file
	- ==Caution==: You must use the "-i" option with care, because the action is not reversible
		- it is always safer to use "sed" without the "-i" option and then replace the file yourself, as shown below:
			- $ sed s/pattern/replace_string/g file1 > file2
		- you can then overwrite file1 with file2 to confirm the changes:
			- $ mv file2 file1
	- eg. To convert 01/02/... to JAN/FEB/...
		- $ sed -e "s/01/JAN/" file
### awk
- "awk" is used to extract and then print specific contents of a file and is often used to construct reports
- create at Bell Labs in 1970 and derives its name from authors: Aho, Weinberger and Kernighan
- "awk" has the ff. features:
	- it is a powerful utility and interpreted programming language
	- it is used to manipulate data files, and for retrieving and processing text
	- it works well with fields (containing a single piece of data, essentially a column)
	- and records (a collection of fields, essentially a line in a file)
- as with "sed", short "awk" commands can be specified directly at the command line, but a more complex script can be saved in a file that you can specify using the "-f" option
- ==common usage==:
	- $ awk 'command' file
		- specify a command directly at the command line
	- $ awk -f scriptfile file
		- specify a file that contains the script to be executed
- ==basic operations==:
	- below explains the basic tasks that can be performed using "awk"
	- the input file is read one line at a time, and, for each line, "awk" matches the given pattern in the given order and performs the requested action
	- the "-F" option allows you to specify a particular "field separator" character
		- eg. the /etc/passwd file uses ':' to separate the fields, so the "-F" option is used with the /etc/passwd file
	- the command/action in "awk" needs to be surrounded with apostrophes (or single-quote ('))
	- "awk" can be used as follows:
		- $ awk '{ print $0 }' /etc/passwd
			- prints the entire file
		- $ awk -F : '{ print $1 }' /etc/passwd
			- prints the first field (column) of every line, separated by a colon
		- $ awk -F : '{ print $1 $7 }' /etc/passwd
			- prints the first & seventh field of every line, separated by a colon
## File Manipulation Utilities
- in managing your files, you may need to perform tasks such as sorting data and copying data from one location to another
- Linux provides numerous file manipulation utilities that you can use while working with text files
### sort
- "sort" is used to ==rearrange the lines of a text file==, in either ascending or descending order ==according to a "sort key"==
	- you can apply the key to sort according to a particular field (column) in a file
	- the default sort key is the order of the ASCII characters (essentially alphabetically)
- ==syntax==:
	- $ sort \<filename\>
		- sort the lines in the specified file, according to the characters at the beginning of each line
	- $ sort file1 file2 | sort
		- combine the two files, then sort the lines and display the output on the terminal
	- $ sort -r \<filename\>
		- sort the lines in reverse order
	- $ sort -k 3 \<filename\>
		- sort the lines by the 3rd field on each line instead of the beginning
- when used with the "-u" option, "sort" checks for unique values after sorting the records (lines)
	- it is equivalent to running "uniq" on the output of sort
### uniq
- "uniq" ==removes duplicate consecutive lines in a text file== and is useful for simplifying the text display
- because "uniq" ==requires that the duplicate entries must be consecutive==, one often ==runs sort first and then pipes the output into "uniq"==
	- if "sort" is used with the "-u" option, it can do all this in one step
- to remove duplicate entries from multiple files at once, use the ff. command:
	- $ sort file1 file2 | uniq > file3
	- or, $sort -u file1 file2 > file3
- to count the number of duplicate entries, use the ff. command:
	- $ uniq -c filename
### paste
- suppose you have a file that contains the full name of all employees and another file that lists their phone numbers and employee IDs
	- you want to create a new file that contains all the data listed in three columns:
		- name, employee ID, phone number
		- how can you do this effectively without investing too much time?
- "paste" can be used to create a single file containing all three columns
- the different columns are identified based on delimiters (spacing used to separate two fields)
	- eg. delimiters can be a blank space, a tab or an "Enter"
- in the image below, a single space is used as the delimiter in all files
- ![[Pasted image 20250612223036.png]]
- "paste" accepts the ff. options:
	- "-d" delimiters
		- specifies a list of delimiters to be used instead of tabs for separating consecutive values on a single line
		- each delimiter is used in turn; when the list has been exhausted, "paste" begins again at the first delimiter
	- "-s"
		- causes paste to append the data in series rather than in parallel
			- ie. horizontally instead of vertically
- Using "paste":
	- "paste" can be used to combine fields (such as name or phone number) from different files, as well as combine lines from multiple files
	- eg. Line one from file1 can be combined with line one of file2, line two from file1 can be combined with line two of file2, and so on
	- to paste contents from two files one can do:
		- $ paste file1 file2
	- the syntax to use a different delimiter is as follows:
		- $ paste -d, file1 file2
		- common delimiters are 'space', 'tab', '|', 'comma', etc.
### join
- suppose you have two files with some similar columns
	- you have saved employees' phone numbers in two files, one with their first name and the other with their last name
		- you want to combine the files without repeating the data of common columns
			- how do you achieve this?
- the above task can be achieved using "join", which is essentially an enhanced version of "paste"
- it first checks whether the files share common fields, such as names or phone numbers, and then joins the lines in two files based on a common field
- using "join":
	- to combine two files on a common field, at the command prompt type:
		- $ join file1 file2
### split
- "split" is used to break up (or split) a file into equal-sized segments for easier viewing and manipulation
- generally used only on relatively large files
- by default, "split" breaks up a file into 1000-line segments
	- the original file remains unchanged, and a set of new files with the same name plus an added prefix is created
		- by default, the 'x' prefix is added
- to split a file into segments, use the command:
	- $ split infile
- to split a file into segments using a different prefix, use the command:
	- $ split infile \<Prefix\>
- using "split":
	- Scenario: we will apply "split" to a dictionary file of almost 500,000 lines
		- $ wc -l linux.words
		- 99171 american-english
			- where we have used "wc" to report on the number of lines in the file
		- $ split linux.words lwords
			- will split the American-English file into equal-sized segments named lwordsxx
## Regular Expressions and Search Patterns
- regular expressions are text strings used for matching a specific pattern, or to search for a specific location, such as the start or end of a line or a word
- regular expressions can contain both normal characters or so-called meta-characters, such as '\*' and '$'
- many text editors and utilities such as "vi, "sed", "awk", "find", and "grep" work extensively with regular expressions
- some of the popular computer languages that use regular expressions include Perl, Python and Ruby
- it can get rather complicated and there are whole books written about regular expressions
- these regular expressions are different from the wildcards (or meta-characters) used in filename matching in command shells such as bash 
- search patterns in regular expressions, and their usage:
	- .(dot): match any single character
	- a|z: match 'a' or 'z'
	- $: match end of a line
	- ^: match beginning of a line
	- \*: match preceding item 0 or more times
- using regex and search patterns:
	- consider the ff. sentence: the quick brown fox jumps over the lazy dog
	- some of the patterns that can be applied to this sentence are as follows:
		- a..
			- matches azy
		- b. | j.
			- matches both br and ju
		- ..$
			- matches og
		- l.\*
			- matches lazy dog
		- l.\*y
			- matches lazy
		- the.\*
			- matches the whole sentence
## grep
- "grep" is extensively used as a primary text searching tool
	- it scans files for specified patterns and can be used with regex, as well as simple strings
- common usages:
	- $ grep \[pattern\] \<filename\>
		- search for a pattern in a file and print al matching lines
	- $ grep -v \[pattern\] \<filename\>
		- print all lines that do not match the pattern
	- $ grep \[0-9\] \<filename\>
		- print the lines that contain the numbers 0 through 9
	- $ grep -C 3 \[pattern\]\<filename\>
		- print context of lines (specified number of lines above and below the pattern) for matching the pattern
## strings
- "strings" is used to extract all printable character strings found in the file or files given as arguments
	- it is useful in locating human-readable content embedded in binary files
		- for text files one can just use "grep"
- eg. to search for the string "my_string" in a spreadsheet:
	- $ strings book1.xls | grep my_string
## Miscellaneous Text Utilities
### tr
- the "tr" utility is used to "translate" specified characters into other characters or to delete them
- general syntax:
	- $ tr \[options\] set1 \[set2\]
	- the items in the square brackets are optional
	- "tr" requires at least one argument and accepts a maximum of two
	- the first, designated as set1 in the example, lists the characters in the text to be replaced or removed
	- the second, set2, lists the characters that are to be substituted for the characters listed in the first argument
	- sometimes, these sets need to be surrounded by apostrophes (or single-quotes (')) in order to have the shell ignore that they mean something special to the shell
		- it is usually safe (and may be required) to use the single-quotes around each of the sets
- eg. suppose you have a file named "city" containing several lines of text in mixed case
	- to translate all lower case characters to upper case
		- at the command line, type:
			- cat city | tr 'a-z' 'A-Z'
- common usage:
	- $ tr 'a-z' 'A-Z'
		- convert lower case to upper case
	- $ tr '{}' '()' \< inputfile \> outputfile
		- translate braces into parenthesis
	- $ echo "This is for testing" | tr \[:space:\] '\t'
		- translate white-space to tabs 
	- $ echo "This is for testing" | tr -s \[:space:\]
		- squeeze (shrink or reduce occurrences) repetition of characters using "-s"
	- $ echo "the geek stuff" | tr -d 't'
		- delete specified characters using "-d" option
	- $ echo "my username is 432234" | tr -cd \[:digit:\]
		- complement the sets using "-c" option
			- ie. in the example, remove characters that are not a digit
	- $ tr -cd \[:print:\] < file.txt
		- remove all non-printable character from a file
	- $ tr -s '\n' ' ' < file.txt
		- squeeze newlines (to include empty lines), translating each into a 'space'
		- join all the lines in a file into a single line
### tee
- takes the output from any command, and, while sending it to standard output, it also saves it to a file
	- ie. it "tees" the output stream from the command:
		- one stream is displayed on the standard output and the other is saved to a file
- eg. to list the contents of a directory on the screen and save the output to a file, type:
	- $ ls -l | tee newfile
### wc
- "wc" (word count) counts the number of lines, words, and characters in a file or list of files
- options:
	- "-l": displays the number of lines
	- "-c": displays the number of bytes
	- "-w": displays the number of words
	- ==by default, all three of these options are active==
### cut
- "cut" is used for manipulating column-based files and is designed to extract specific columns
- the default column separator is the "TAB" character
- a different delimiter can be given as a command option
- eg. to display the third column delimited by "blank space", type:
	- $ ls -l | cud -d " " -f3